# ‚òÅÔ∏èüïí MCP - Local Ollama - Asistente de Clima y Hora

Este proyecto es una prueba de concepto que demuestra c√≥mo integrar un **Model Context Protocol (MCP)** basado en TypeScript con un agente de IA de **PraisonAI**, utilizando un modelo de lenguaje local (`gemma3:12b`) ejecutado con **Ollama** y una interfaz de usuario creada con **Gradio**.

El asistente permite a los usuarios consultar la hora y el clima de ubicaciones espec√≠ficas, extendiendo las capacidades del LLM mediante herramientas personalizadas.

---

## üõ†Ô∏è Tecnolog√≠as Utilizadas

* **PraisonAI Agents**: Framework para construir agentes de IA con capacidades de herramientas.
* **Model Context Protocol (MCP)**: Est√°ndar para que los modelos de lenguaje accedan a herramientas externas y contexto.
* **Ollama**: Plataforma para ejecutar modelos de lenguaje grandes (LLMs) localmente.
* **Gradio**: Librer√≠a de Python para crear interfaces de usuario web r√°pidas para modelos de ML.
* **TypeScript**: Lenguaje de programaci√≥n para el desarrollo del servidor MCP.
* **Node.js / npm**: Entorno de ejecuci√≥n y gestor de paquetes para el servidor MCP.
* **worldtimeapi.org**: API p√∫blica utilizada para obtener la hora global.
* **Open-Meteo**: API p√∫blica utilizada para obtener datos meteorol√≥gicos.

---

## üöÄ De qu√© trata la prueba

Esta prueba valida la capacidad de un agente de IA para:
1.  **Interactuar con herramientas externas**: Utilizando un servidor MCP personalizado para acceder a datos de tiempo y clima en tiempo real.
2.  **Ejecutar modelos de lenguaje localmente**: Demostrando la integraci√≥n con Ollama para mantener el procesamiento de IA en una maquina local.
3.  **Proporcionar una interfaz de usuario interactiva**: A trav√©s de Gradio, permitiendo a los usuarios hacer consultas de forma sencilla.

El objetivo es mostrar un flujo completo donde un usuario pregunta sobre el clima o la hora, el agente utiliza las herramientas del MCP para obtener la informaci√≥n necesaria y luego responde al usuario.

---

## ‚öôÔ∏è Paso a paso para instalar y ejecutar

Aseg√∫rate de tener **Node.js**, **npm**, **Python 3.9+** y **Ollama** instalados en tu sistema.

### 1. Clonar el repositorio

```bash
git clone https://github.com/Juliotamara23/MCP_Weather_Hour.git>
cd <MCP_Weather_Hour>
```

### 2. Configurar el servidor MCP (TypeScript)

**En proceso**

### 3. Instalar Ollama y descargar el modelo

Aseg√∫rate de tener Ollama instalado y ejecut√°ndose. Luego, descarga el modelo gemma3:12b:

```bash
ollama pull gemma3:12b
```

### 4. Configurar el entorno de Python

Crea un entorno virtual (opcional pero recomendado) e instala las dependencias de Python:

```bash
python -m venv .venv
source .venv/bin/activate  # En Windows
source .venv/Scripts/activate  # En Linux/Mac

pip install -r requirements.txt
```

### 5. Ejecutar la aplicaci√≥n Gradio

```bash
python main.py
```